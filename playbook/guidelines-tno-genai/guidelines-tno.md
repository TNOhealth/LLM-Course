# TNO-HLW Guidelines for Generative A.I. use

</br>

A full guideline for the use of Generative AI within TNO broadly is currently being developed. This guideline will deal with exploiting the Potential, the additional Risks and the Responsibilities regarding the use of GenAI and LLMs. This guideline is expected around summer (2024). As these tools are already in full use and we can now also use Copilot as a standard LLM within our working environment, it is important to already pay attention to the risks of using GenAI and LLMs such as:

*  Content risks, such as the possible unreliability and/or incompleteness of information;
*  Privacy issues, e.g. due to re-identification;
*  Ethical issues, e.g. due to bias in the underlying data used for training the LLM;
*  Disclosure of confidential information, e.g. due to information entered or questions asked.

Therefore, for now, the following arrangements apply to the use of GenAI and LLMs within HLW

1.	Input: when data, information or other input is entered into an AIa or LLM, it is necessary to establish beforehand whether this is allowed, e.g. because of privacy, security or competitive position:
*  All data owned by customers, partners, or other external parties and all data with IP rights that do not belong to TNO can only be entered into AIa or LLMs when customers, partners or the owner of the rights have given explicit permission.
*  All data containing TNO knowledge that may not be freely shared with the outside world can only be entered into AIa or LLMs after the MD of the unit has given permission to do so - following legal advice from TNO IP&C.
*  The operation and limitations of that particular AIa or LLM must be sufficiently understood by the authors, so that the context or bandwidth of its results are also sufficiently known;
2.	Output: information generated by AIa or LLMs is not reliable, changes over time and is not always complete. Thus, it can only be used for support. It should be carefully reported how the method was applied and when it was applied;
3.	Responsibility: the user remains responsible for the results; ChatGPT (or other LLMs) cannot be considered the author.


```{figure} ./_static/img/datastrategy1.png
---
height: 464px
name: datastrategy1
---
```
 
 </br>

:::{seealso}

*Bringing Code to Data: Do Not Forget Governance*, article available on [PubMed](https://pubmed.ncbi.nlm.nih.gov/32540846/).

:::


