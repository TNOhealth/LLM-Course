# What are Large Language Models

</br>

But first start with the question what are Large Language Models (LLMs)? So let’s start. You have probably heard of the application ChatGPT that is able to answer questions, translate text and to have human like conversations. ChatGPT is built upon a LLM, GPT-3 or GPT-4, an algorithm that enables ChatGPT to perform it’s tricks. So how can such an algorithm do this? Imagine you have a parrot that's been around people its entire life, and it's heard countless conversations about a huge array of topics. Over time, it's learned to mimic human speech pretty convincingly, to the point where it can respond to your comments or questions with relevant phrases it's heard before. If you ask it "How's the weather?", it might reply with "It's raining cats and dogs!" because it's heard that phrase used to describe heavy rain.

However, it's important to remember that this parrot doesn't actually understand what it's saying. It doesn't know what "weather" is or what "raining cats and dogs" means. It's simply learned that certain responses are likely to follow certain prompts, based on the patterns it's observed in human conversation.

In a similar way, Large Language Models (LLMs) like OpenAI's GPT-4 are AI systems that have been trained on a massive amount of text data and code (see figure 1). They learn the patterns, structures, and common phrases in the language, and use this knowledge to generate human-like text based on a given prompt. However, like the parrot, they don't truly understand the meaning of the text they generate - they're predicting the most likely response based on their training.
In the context of a course on prompt engineering, this understanding is crucial. When crafting prompts for an LLM, it's important to keep in mind that the model is trying to predict a likely response based on its training, not truly understanding the prompt or the implications of its response.

## ChatGPT-4 example

And we have taken off! Actually this piece was (mostly) written by ChatGPT-4 (see the screenshot below). The block of the green A represents the ‘prompt’ and the purple block represents the answer to this prompt. The main message of ChatGPT is that LLMs are not ‘smart’ or ‘intelligent’, in fact LLMs are complex statistical models that are trained on a massive amount of text that have been collected from the world wide web, this includes online books and all kind of web-pages. These models have the capacity to generating human-like text in response to prompt. 


```{figure} ./_static/img/llm_fig2.png
---
height: 464px
name: ChatGPT example
---
```

 </br>

:::{note}
a critical piece about generative A.I. and LLMs in particular you can find here [here](https://pubmed.ncbi.nlm.nih.gov/32540846) . If you want to get in more dept about how ChatGPT was built, wolfram alpha has a very good and readable [article](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)
:::

